{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment #2.2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manojec054/Assignment/blob/master/Assignment_2_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIUtlDHgshBq"
      },
      "source": [
        "This the next assignment. Provided is the backprop code in python. You need to write the rest of program to classify iris dataset. Write the structure of neural net, training, loss generation, plot the loss and experiment with different parameters and network configuration.  Try to write a legible code with lots of comments wherever necessary. Submit the working code to amritansh.48@gmail.com. Just share the Google colab link, makes it easier to debug. \n",
        "\n",
        "Have fun writing and debugging the code.!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyzBny-LqLd3"
      },
      "source": [
        "class Network(object):\n",
        "    # ADDED: taking weight, bias num_layers\n",
        "    def __init__(self, weight, bias, num_layers):\n",
        "        self.weights = weight\n",
        "        self.biases = bias\n",
        "        self.num_layers = num_layers\n",
        "    \n",
        "    def update_weight_bias(self, weight, bias):\n",
        "        self.weights = weight\n",
        "        self.biases = bias\n",
        "\n",
        "    def backprop(self, x, y):\n",
        "        \"\"\"Return a tuple \"(nabla_b, nabla_w)\" representing the\n",
        "        gradient for the cost function C_x.  \"nabla_b\" and\n",
        "        \"nabla_w\" are layer-by-layer lists of numpy arrays, similar\n",
        "        to \"self.biases\" and \"self.weights\".\"\"\"\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "\n",
        "        # feedforward\n",
        "        activation = x\n",
        "        activations = [x] # list to store all the activations, layer by layer\n",
        "        zs = [] # list to store all the z vectors, layer by layer\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation)+b\n",
        "            zs.append(z)\n",
        "            activation = self.sigmoid(z)\n",
        "            activations.append(activation)        \n",
        "                    \n",
        "        # backward pass\n",
        "        delta = self.cost_derivative(activations[-1], y) * self.sigmoid_prime(zs[-1])\n",
        "        nabla_b[-1] = delta\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "\n",
        "\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l]\n",
        "            sp = self.sigmoid_prime(z)\n",
        "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
        "            nabla_b[-l] = delta\n",
        "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "        return (nabla_b, nabla_w, self.calculate_loss(activations[-1], y))\n",
        "    \n",
        "    # ADDED takes x as input and provide predicted output\n",
        "    def predict(self, x):\n",
        "        activation = x\n",
        "        activations = [x] # list to store all the activations, layer by layer\n",
        "        zs = [] # list to store all the z vectors, layer by layer\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation)+b\n",
        "            zs.append(z)\n",
        "            activation = self.sigmoid(z)\n",
        "            activations.append(activation) \n",
        "        \n",
        "        return np.argmax(activation)\n",
        "\n",
        "\n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
        "        \\partial a for the output activations.\"\"\"\n",
        "        return (output_activations-y) \n",
        "      \n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"The sigmoid function.\"\"\"\n",
        "        return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "    def sigmoid_prime(self, z):\n",
        "        \"\"\"Derivative of the sigmoid function.\"\"\"\n",
        "        return self.sigmoid(z)*(1-self.sigmoid(z))\n",
        "    \n",
        "    def calculate_loss(self, prob, y):\n",
        "        from sklearn.metrics import log_loss\n",
        "        return log_loss(y, prob)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Your code from here\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_iris # only to load iris dataset\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score # for model evaluation\n",
        "iris_data = load_iris()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ADDED: Created layer class to add layers \n",
        "class Layer():\n",
        "    def __init__(self):\n",
        "        self.weight = []\n",
        "        self.bias = []\n",
        "        self.hidden_layer = 1 # Included input layer\n",
        "    \n",
        "    def add_layer(self, total_neurons, neuron_in_prev_layer):\n",
        "        #np.random.seed(100)\n",
        "        self.weight.append(np.random.random((total_neurons, neuron_in_prev_layer)))\n",
        "        self.bias.append(np.random.random((total_neurons, 1)))\n",
        "        self.hidden_layer += 1\n",
        "    \n",
        "    def get_layer_weight_bias(self, layer_num):\n",
        "        if layer_num > len(self.weight):\n",
        "            print(f\"ERROR: Only have {len(self.weight)} layers\")\n",
        "        else:\n",
        "            return (self.weight[layer_num - 1], self.bias[layer_num - 1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.DataFrame(data=iris_data.data, columns=iris_data.feature_names)\n",
        "df['target'] = iris_data.target\n",
        "\n",
        "output_layer_neuron = df.target.value_counts().shape[0]\n",
        "input_layer_neuron = iris_data.data.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
              "0                5.1               3.5                1.4               0.2   \n",
              "1                4.9               3.0                1.4               0.2   \n",
              "2                4.7               3.2                1.3               0.2   \n",
              "3                4.6               3.1                1.5               0.2   \n",
              "4                5.0               3.6                1.4               0.2   \n",
              "\n",
              "   target_0  target_1  target_2  \n",
              "0         1         0         0  \n",
              "1         1         0         0  \n",
              "2         1         0         0  \n",
              "3         1         0         0  \n",
              "4         1         0         0  "
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal length (cm)</th>\n      <th>sepal width (cm)</th>\n      <th>petal length (cm)</th>\n      <th>petal width (cm)</th>\n      <th>target_0</th>\n      <th>target_1</th>\n      <th>target_2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# dummies is created to get proper shaped values in output neuron,\n",
        "# EX: y = 1 = [0,1,0]\n",
        "#     y = 0 = [1,0,0]\n",
        "df = pd.get_dummies(df, columns=['target'], prefix='target')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "layer = Layer()\n",
        "layer.add_layer(8, input_layer_neuron)       # Hidden layer 1\n",
        "layer.add_layer(10, 8)                       # Hidden layer 2\n",
        "layer.add_layer(output_layer_neuron, 10)     # output layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.loc[:,:'petal width (cm)'], df.iloc[:,4:], random_state=100, test_size=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "nw = Network(layer.weight, layer.bias, layer.hidden_layer)\n",
        "loss_list = []\n",
        "epoch = 500\n",
        "learning_rate = 0.5\n",
        "for i in range(0, epoch):\n",
        "    for row, x in enumerate(range(0, X_train.shape[0])):\n",
        "        x = X_train.iloc[row].values.reshape(4, 1)\n",
        "        y = y_train.iloc[row].values.reshape(3,1)\n",
        "        grad_bias, grad_weight, loss = nw.backprop(x, y)\n",
        "        layer.weight = layer.weight - (learning_rate * np.array(grad_weight))\n",
        "        layer.bias = layer.bias - (learning_rate * np.array(grad_bias))\n",
        "        nw.update_weight_bias(layer.weight, layer.bias)\n",
        "    \n",
        "    loss_list.append(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x1a3e1ff9148>]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 432x288 with 1 Axes>",
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 372.103125 248.518125\" width=\"372.103125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;white-space:pre;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 372.103125 248.518125 \r\nL 372.103125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 364.903125 224.64 \r\nL 364.903125 7.2 \r\nL 30.103125 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m50149f5827\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.321307\" xlink:href=\"#m50149f5827\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(42.140057 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"106.316024\" xlink:href=\"#m50149f5827\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 100 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(96.772274 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"167.31074\" xlink:href=\"#m50149f5827\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 200 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(157.76699 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"228.305457\" xlink:href=\"#m50149f5827\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 300 -->\r\n      <defs>\r\n       <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n      </defs>\r\n      <g transform=\"translate(218.761707 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"289.300174\" xlink:href=\"#m50149f5827\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 400 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(279.756424 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"350.29489\" xlink:href=\"#m50149f5827\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 500 -->\r\n      <defs>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(340.75114 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"mdacbacbfbc\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mdacbacbfbc\" y=\"215.198458\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0.0 -->\r\n      <defs>\r\n       <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 218.997676)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mdacbacbfbc\" y=\"179.779595\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.5 -->\r\n      <g transform=\"translate(7.2 183.578814)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mdacbacbfbc\" y=\"144.360733\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 1.0 -->\r\n      <g transform=\"translate(7.2 148.159952)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mdacbacbfbc\" y=\"108.941871\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 1.5 -->\r\n      <g transform=\"translate(7.2 112.74109)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mdacbacbfbc\" y=\"73.523009\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 2.0 -->\r\n      <g transform=\"translate(7.2 77.322227)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#mdacbacbfbc\" y=\"38.104146\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 2.5 -->\r\n      <g transform=\"translate(7.2 41.903365)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_13\">\r\n    <path clip-path=\"url(#p60f69de269)\" d=\"M 45.321307 17.083636 \r\nL 45.931254 147.195259 \r\nL 46.541201 147.582046 \r\nL 50.810831 147.742181 \r\nL 52.030726 147.895561 \r\nL 52.640673 148.046594 \r\nL 53.25062 148.328248 \r\nL 53.860567 148.848317 \r\nL 54.470514 149.985614 \r\nL 55.080461 152.656076 \r\nL 55.690409 159.2176 \r\nL 56.910303 188.754543 \r\nL 57.52025 196.672146 \r\nL 58.130197 200.257548 \r\nL 58.740144 202.598799 \r\nL 59.350092 204.333065 \r\nL 59.960039 205.641469 \r\nL 60.569986 206.635962 \r\nL 61.179933 207.396179 \r\nL 62.399827 208.416741 \r\nL 63.619722 208.990287 \r\nL 65.449563 209.450426 \r\nL 67.279405 209.628653 \r\nL 69.109246 209.492793 \r\nL 70.939088 209.26623 \r\nL 71.549035 209.343463 \r\nL 72.158982 209.842688 \r\nL 72.768929 209.599157 \r\nL 73.378877 210.198612 \r\nL 73.988824 210.566618 \r\nL 74.598771 210.589355 \r\nL 75.208718 211.099955 \r\nL 75.818665 210.014766 \r\nL 76.428612 211.454247 \r\nL 77.03856 211.013664 \r\nL 77.648507 211.631661 \r\nL 78.258454 210.441794 \r\nL 78.868401 210.351535 \r\nL 79.478348 212.094242 \r\nL 80.088295 210.812265 \r\nL 80.698243 212.437239 \r\nL 81.30819 212.185736 \r\nL 81.918137 212.606367 \r\nL 82.528084 209.87303 \r\nL 83.138031 212.843766 \r\nL 83.747978 212.684698 \r\nL 84.357926 212.158599 \r\nL 85.57782 212.509554 \r\nL 86.187767 212.866989 \r\nL 86.797714 212.865315 \r\nL 87.407661 211.538204 \r\nL 88.017609 212.965171 \r\nL 88.627556 213.060378 \r\nL 89.237503 212.925312 \r\nL 89.84745 212.267929 \r\nL 90.457397 212.90482 \r\nL 91.067344 213.351884 \r\nL 91.677292 212.678166 \r\nL 92.287239 213.368086 \r\nL 92.897186 213.503934 \r\nL 93.507133 213.008288 \r\nL 94.727027 213.489481 \r\nL 95.336975 212.897903 \r\nL 95.946922 213.132123 \r\nL 96.556869 209.680824 \r\nL 97.166816 211.048057 \r\nL 97.776763 211.758955 \r\nL 98.996658 212.721578 \r\nL 99.606605 214.103406 \r\nL 100.826499 213.810304 \r\nL 101.436446 213.877559 \r\nL 102.656341 213.795182 \r\nL 104.486182 214.005949 \r\nL 105.096129 211.520656 \r\nL 105.706076 213.018878 \r\nL 106.316024 214.095071 \r\nL 108.145865 214.007447 \r\nL 108.755812 214.156626 \r\nL 109.365759 214.14514 \r\nL 109.975707 213.94394 \r\nL 110.585654 214.023788 \r\nL 111.195601 212.116928 \r\nL 111.805548 214.138343 \r\nL 112.415495 214.129445 \r\nL 113.025442 214.321497 \r\nL 114.245337 214.090309 \r\nL 114.855284 214.195194 \r\nL 115.465231 213.999299 \r\nL 116.075178 213.960247 \r\nL 116.685125 213.669932 \r\nL 117.295073 214.063464 \r\nL 117.90502 214.163163 \r\nL 118.514967 212.847725 \r\nL 119.124914 214.135905 \r\nL 119.734861 214.032636 \r\nL 120.344808 214.203806 \r\nL 120.954756 214.124357 \r\nL 121.564703 214.218506 \r\nL 122.17465 214.003571 \r\nL 122.784597 214.20546 \r\nL 123.394544 213.986338 \r\nL 127.664174 214.233568 \r\nL 128.274122 214.228381 \r\nL 128.884069 214.090271 \r\nL 129.494016 214.229461 \r\nL 131.323857 214.041746 \r\nL 133.763646 213.930192 \r\nL 139.863118 213.90553 \r\nL 140.473065 214.185797 \r\nL 141.083012 214.079447 \r\nL 141.692959 214.123182 \r\nL 142.302906 214.020481 \r\nL 142.912854 214.261016 \r\nL 144.742695 214.2524 \r\nL 145.352642 214.158326 \r\nL 146.572537 214.133077 \r\nL 150.842167 213.922426 \r\nL 151.452114 214.019554 \r\nL 154.50185 214.02483 \r\nL 155.111797 214.079656 \r\nL 155.721744 213.90493 \r\nL 156.331691 214.153906 \r\nL 156.941638 214.052213 \r\nL 158.161533 214.216921 \r\nL 158.77148 214.224111 \r\nL 159.381427 214.104101 \r\nL 159.991374 214.226218 \r\nL 160.601321 214.115155 \r\nL 163.651057 214.288669 \r\nL 165.480899 214.232415 \r\nL 166.090846 214.282357 \r\nL 166.700793 214.155034 \r\nL 167.31074 214.281945 \r\nL 167.920687 214.06442 \r\nL 169.140582 214.333541 \r\nL 169.750529 214.225099 \r\nL 170.360476 214.348266 \r\nL 171.58037 214.302493 \r\nL 172.190318 214.093504 \r\nL 174.020159 214.304028 \r\nL 175.240053 214.129404 \r\nL 175.850001 214.312634 \r\nL 180.729578 214.136017 \r\nL 181.339525 214.246306 \r\nL 181.949472 214.128921 \r\nL 183.169367 214.18666 \r\nL 183.779314 214.052474 \r\nL 184.389261 214.292927 \r\nL 184.999208 214.117339 \r\nL 185.609155 214.337052 \r\nL 186.219102 214.256356 \r\nL 186.82905 214.429639 \r\nL 188.658891 214.384918 \r\nL 189.268838 214.214875 \r\nL 190.488733 214.38765 \r\nL 194.148416 214.197812 \r\nL 195.36831 214.156053 \r\nL 195.978257 214.298703 \r\nL 196.588204 214.217421 \r\nL 197.808099 214.291345 \r\nL 199.027993 214.108101 \r\nL 199.63794 214.273166 \r\nL 200.857834 214.202539 \r\nL 202.687676 214.038128 \r\nL 203.297623 213.989289 \r\nL 203.90757 214.062176 \r\nL 204.517517 213.887291 \r\nL 205.127465 214.074106 \r\nL 205.737412 213.94702 \r\nL 206.347359 214.143912 \r\nL 206.957306 214.083155 \r\nL 207.567253 213.910701 \r\nL 208.1772 214.337082 \r\nL 208.787148 214.366969 \r\nL 210.616989 214.118624 \r\nL 214.276672 214.246081 \r\nL 214.886619 214.084113 \r\nL 215.496566 214.165108 \r\nL 216.106514 213.943523 \r\nL 216.716461 214.17117 \r\nL 217.326408 214.062792 \r\nL 218.546302 214.407454 \r\nL 219.156249 214.078087 \r\nL 219.766197 214.476425 \r\nL 220.986091 214.394841 \r\nL 224.035827 214.349413 \r\nL 224.645774 214.192089 \r\nL 225.255721 214.341888 \r\nL 225.865668 214.375479 \r\nL 226.475615 214.059054 \r\nL 227.085563 213.993319 \r\nL 228.305457 214.418463 \r\nL 228.915404 214.244268 \r\nL 229.525351 214.281994 \r\nL 230.745246 214.209128 \r\nL 231.355193 214.176619 \r\nL 231.96514 214.537128 \r\nL 232.575087 214.506664 \r\nL 233.185034 214.206832 \r\nL 233.794981 214.491776 \r\nL 235.014876 214.518885 \r\nL 235.624823 214.307157 \r\nL 236.844717 214.141035 \r\nL 237.454664 214.448248 \r\nL 240.5044 214.565352 \r\nL 241.114347 214.282025 \r\nL 241.724295 214.527012 \r\nL 242.334242 214.599104 \r\nL 242.944189 214.427618 \r\nL 244.164083 214.552526 \r\nL 244.77403 214.47951 \r\nL 245.993925 214.542809 \r\nL 247.213819 214.233079 \r\nL 247.823766 214.580337 \r\nL 248.433713 214.407318 \r\nL 249.043661 214.629152 \r\nL 250.873502 214.427563 \r\nL 251.483449 214.292859 \r\nL 252.093396 214.559327 \r\nL 252.703344 214.359519 \r\nL 253.923238 214.493624 \r\nL 254.533185 214.5563 \r\nL 255.143132 214.30533 \r\nL 255.753079 214.534952 \r\nL 256.363027 214.309194 \r\nL 256.972974 214.606671 \r\nL 257.582921 214.419884 \r\nL 258.192868 214.562007 \r\nL 258.802815 214.397162 \r\nL 260.02271 214.428182 \r\nL 260.632657 214.526589 \r\nL 261.242604 214.398889 \r\nL 261.852551 214.622363 \r\nL 263.682393 214.226171 \r\nL 264.29234 214.556406 \r\nL 266.122181 214.598526 \r\nL 267.342076 214.342863 \r\nL 273.441547 214.585028 \r\nL 274.051494 214.461556 \r\nL 278.321125 214.613968 \r\nL 278.931072 214.545805 \r\nL 280.150966 214.677931 \r\nL 281.37086 214.564822 \r\nL 281.980808 214.47689 \r\nL 282.590755 214.552124 \r\nL 283.200702 214.503666 \r\nL 283.810649 214.65236 \r\nL 285.640491 214.575838 \r\nL 286.250438 214.640338 \r\nL 286.860385 214.455428 \r\nL 289.910121 214.613273 \r\nL 290.520068 214.687411 \r\nL 291.739962 214.553151 \r\nL 292.349909 214.266993 \r\nL 292.959857 214.578858 \r\nL 294.179751 214.486604 \r\nL 294.789698 214.252424 \r\nL 295.399645 214.695533 \r\nL 296.009592 214.607944 \r\nL 296.61954 214.664991 \r\nL 297.229487 214.582894 \r\nL 298.449381 214.756364 \r\nL 299.669275 214.549978 \r\nL 301.499117 214.707824 \r\nL 303.938906 214.7063 \r\nL 306.988641 214.596591 \r\nL 308.818483 214.628823 \r\nL 310.038377 214.556757 \r\nL 313.088113 214.725065 \r\nL 314.308007 214.603009 \r\nL 314.917955 214.553712 \r\nL 316.137849 214.642917 \r\nL 317.96769 214.666032 \r\nL 319.187585 214.656046 \r\nL 321.017426 214.655203 \r\nL 321.627373 214.437423 \r\nL 322.237321 214.583039 \r\nL 323.457215 214.455679 \r\nL 324.067162 214.671359 \r\nL 324.677109 214.529323 \r\nL 328.94674 214.682754 \r\nL 330.166634 214.555891 \r\nL 330.776581 214.341991 \r\nL 331.386528 214.557771 \r\nL 332.606423 214.583001 \r\nL 338.095947 214.546357 \r\nL 338.705894 214.666005 \r\nL 344.805366 214.607956 \r\nL 347.245155 214.672805 \r\nL 349.074996 214.644037 \r\nL 349.684943 214.609664 \r\nL 349.684943 214.609664 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 30.103125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 364.903125 224.64 \r\nL 364.903125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 364.903125 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 30.103125 7.2 \r\nL 364.903125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p60f69de269\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"30.103125\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt   \n",
        "plt.plot(loss_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred = []\n",
        "for i, row in enumerate(range(0, X_test.shape[0])):\n",
        "    x = X_test.iloc[row].values.reshape(4, 1)\n",
        "    pred.append(nw.predict(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[16,  0,  0],\n",
              "       [ 0, 11,  0],\n",
              "       [ 0,  0, 18]], dtype=int64)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "confusion_matrix(np.array(pred), np.argmax(y_test.values, axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "\n",
        "accuracy_score(np.array(pred), np.argmax(y_test.values, axis=1))"
      ]
    }
  ]
}